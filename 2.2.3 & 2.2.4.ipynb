{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_matrix_txt(path: str):\n",
    "\n",
    "    H = np.loadtxt(path, dtype=int)\n",
    "    H = H % 2\n",
    "    if H.ndim == 1:\n",
    "        # If a single row is loaded, make it 2D\n",
    "        H = H.reshape(1, -1)\n",
    "    return H\n",
    "\n",
    "\n",
    "def load_vector_txt(path: str):\n",
    " \n",
    "    y = np.loadtxt(path, dtype=int)\n",
    "    y = np.asarray(y).reshape(-1)  # flatten\n",
    "    y = y % 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def syndrome(H: np.ndarray, x: np.ndarray):\n",
    "    \n",
    "    return (H @ x) % 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded vector x_hat:\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "Return code: 0 (0=success, -1=not converged)\n",
      "Iterations used: 8\n"
     ]
    }
   ],
   "source": [
    "# LDPC decoder (Loopy BP / Sum-Product) for BSC in LLR domain\n",
    "def ldpc_decode_bsc(H_hat, y, p, max_iter):\n",
    "\n",
    "    H = (H_hat.astype(int) % 2)\n",
    "    y = (np.asarray(y).astype(int).reshape(-1) % 2)\n",
    "\n",
    "    M, N = H.shape\n",
    "    if y.shape[0] != N:\n",
    "        raise ValueError(f\"y has length {y.shape[0]} but H has {N} columns.\")\n",
    "\n",
    "    # Build adjacency lists\n",
    "    check_neighbors = [np.where(H[m, :] == 1)[0] for m in range(M)]  # vars per check\n",
    "    var_neighbors   = [np.where(H[:, i] == 1)[0] for i in range(N)]  # checks per var\n",
    "\n",
    "    # For fast index mapping: pos_in_check[m][i] = position of variable i in check_neighbors[m]\n",
    "    pos_in_check = [dict() for _ in range(M)]\n",
    "    for m in range(M):\n",
    "        for k, i in enumerate(check_neighbors[m]):\n",
    "            pos_in_check[m][i] = k\n",
    "\n",
    "    # Channel LLRs for BSC:\n",
    "    # Lch_i = log( P(y_i|x_i=0) / P(y_i|x_i=1) )\n",
    "    # If y_i=0: Lch = log((1-p)/p), if y_i=1: Lch = -log((1-p)/p)\n",
    "    L0 = np.log((1.0 - p) / p)\n",
    "    Lch = np.where(y == 0, L0, -L0).astype(float)\n",
    "\n",
    "    # Messages stored per edge in \"check-major\" format:\n",
    "    # Lq[m][k] = LLR message variable->check along edge (check m, its k-th neighbor variable)\n",
    "    # Lr[m][k] = LLR message check->variable along same edge\n",
    "    Lq = [Lch[check_neighbors[m]].copy() for m in range(M)]\n",
    "    Lr = [np.zeros(len(check_neighbors[m]), dtype=float) for m in range(M)]\n",
    "\n",
    "    # Iterations\n",
    "    x_hat = np.zeros(N, dtype=int)\n",
    "\n",
    "    # small helper for numeric safety\n",
    "    def clamp_unit(v, eps=1e-12):\n",
    "        return np.clip(v, -1.0 + eps, 1.0 - eps)\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "\n",
    "        # (1) Check-to-variable update (Lr)\n",
    "        # Lr = 2 * atanh( prod_{j≠i} tanh(Lq/2) )\n",
    "        for m in range(M):\n",
    "            deg = len(check_neighbors[m])\n",
    "            if deg == 0:\n",
    "                continue\n",
    "\n",
    "            t = np.tanh(Lq[m] / 2.0)  # shape (deg,)\n",
    "            total_prod = np.prod(t)\n",
    "\n",
    "            for k in range(deg):\n",
    "                # product excluding k\n",
    "                if abs(t[k]) > 1e-15:\n",
    "                    prod_excl = total_prod / t[k]\n",
    "                else:\n",
    "                    # fallback: compute explicitly excluding k to avoid divide by ~0\n",
    "                    prod_excl = np.prod(np.delete(t, k))\n",
    "\n",
    "                prod_excl = clamp_unit(prod_excl)\n",
    "                Lr[m][k] = 2.0 * np.arctanh(prod_excl)\n",
    "\n",
    "        # (2) Variable-to-check update (Lq)\n",
    "        # Lq_{i->m} = Lch_i + sum_{m'∈N(i)\\{m}} Lr_{m'->i}\n",
    "        for i in range(N):\n",
    "            checks = var_neighbors[i]\n",
    "            if checks.size == 0:\n",
    "                continue\n",
    "\n",
    "            # sum of incoming Lr from all checks to variable i\n",
    "            incoming_sum = 0.0\n",
    "            for m in checks:\n",
    "                k = pos_in_check[m][i]\n",
    "                incoming_sum += Lr[m][k]\n",
    "\n",
    "            # update each outgoing message to each check\n",
    "            for m in checks:\n",
    "                k = pos_in_check[m][i]\n",
    "                Lq[m][k] = Lch[i] + (incoming_sum - Lr[m][k])\n",
    "\n",
    "        # (3) Compute posterior LLRs and hard decision\n",
    "        # Lpost_i = Lch_i + sum_{m∈N(i)} Lr_{m->i}\n",
    "        # decide x_hat_i = 0 if Lpost>0 else 1\n",
    "        for i in range(N):\n",
    "            Lpost = Lch[i]\n",
    "            for m in var_neighbors[i]:\n",
    "                k = pos_in_check[m][i]\n",
    "                Lpost += Lr[m][k]\n",
    "            x_hat[i] = 1 if (Lpost < 0) else 0\n",
    "\n",
    "        # (4) Stop if syndrome is zero\n",
    "        if np.all(syndrome(H, x_hat) == 0):\n",
    "            return x_hat, 0, it\n",
    "\n",
    "    return x_hat, -1, max_iter\n",
    "\n",
    "\n",
    "# Run the required experiment\n",
    "def run_assignment3(H_path=\"H1.txt\", y_path=\"y1.txt\", p=0.1, max_iter=20):\n",
    "    H1 = load_matrix_txt(H_path)\n",
    "    y1 = load_vector_txt(y_path)\n",
    "\n",
    "    x_hat, code, iters = ldpc_decode_bsc(H1, y1, p, max_iter=max_iter)\n",
    "\n",
    "    print(\"Decoded vector x_hat:\")\n",
    "    print(x_hat.tolist())\n",
    "    print(\"Return code:\", code, \"(0=success, -1=not converged)\")\n",
    "    print(\"Iterations used:\", iters)\n",
    "    return x_hat, code, iters\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_assignment3(\"H1.txt\", \"y1.txt\", p=0.1, max_iter=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01161ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded vector x_hat:\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "Return code: 0 (0=success, -1=not converged)\n",
      "Iterations used: 8\n",
      "Decoded message (MSB-first): 'Happy Holidays! Dmitry&David :)'\n",
      "Decoded message (LSB-first): '\\x12\\x86\\x0e\\x0e\\x9e\\x04\\x12ö6\\x96&\\x86\\x9eÎ\\x84\\x04\"¶\\x96.N\\x9ed\"\\x86n\\x96&\\x04\\\\\\x94'\n"
     ]
    }
   ],
   "source": [
    "x_hat, code, iters = run_assignment3(\"H1.txt\", \"y1.txt\", p=0.1, max_iter=20)\n",
    "\n",
    "\n",
    "def bits_to_ascii(bits, msb_first=True):\n",
    "    bits = list(bits)\n",
    "    out = []\n",
    "    for i in range(0, len(bits), 8):\n",
    "        byte = bits[i:i+8]\n",
    "        if not msb_first:\n",
    "            byte = byte[::-1]\n",
    "        val = int(\"\".join(map(str, byte)), 2)\n",
    "        out.append(chr(val))\n",
    "    return \"\".join(out)\n",
    "\n",
    "bits_248 = x_hat[:248]\n",
    "\n",
    "msg = bits_to_ascii(bits_248, msb_first=True)\n",
    "print(\"Decoded message (MSB-first):\", repr(msg))\n",
    "\n",
    "msg2 = bits_to_ascii(bits_248, msb_first=False)\n",
    "print(\"Decoded message (LSB-first):\", repr(msg2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
